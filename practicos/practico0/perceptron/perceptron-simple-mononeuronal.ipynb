{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refs.\n",
    "\n",
    "1. Introduction to the theory of neuronal computation, Hertz, Krogh, Palmer (1991)\n",
    "\n",
    "2. https://machinelearningmastery.com/implement-perceptron-algorithm-scratch-python/\n",
    "\n",
    "3. https://towardsdatascience.com/perceptron-explanation-implementation-and-a-visual-example-3c8e76b4e2d1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teoría\n",
    "\n",
    "### Una neurona de salida\n",
    "\n",
    "Consideramos un perceptrón simple (una capa) de $n$ entradas y de una salida.\n",
    "Si el vector $x\\in\\mathbb{R}^n$ representa el estado de las neuronas de entrada, el estado de la neurona de salida viene determinado por\n",
    "\n",
    "$$ g(b+wx) \\;\\;\\; (1)$$\n",
    "\n",
    "donde la constante $b\\in \\mathbb{R}$ se llama umbral, y el vector $w\\in \\mathbb{R}^n$ representa los pesos sinápticos.\n",
    "Aquí\n",
    "\n",
    "$$ wx = \\Sigma_{i=1}^n w_ix_i $$ \n",
    "\n",
    "es un producto escalar y $g\\in (\\mathbb{R} \\to \\mathbb{R})$ es alguna función activación.\n",
    "Básicamente, las funciones activación son funciones no decrecientes que poseen alguna no linealidad.\n",
    "Existen muchas funciones de activación, pero nos enfocaremos en usar\n",
    "\n",
    "\\begin{eqnarray}\n",
    "g(x)\n",
    "&=& \\tanh(\\beta x) \\\\\n",
    "&=& \\frac{e^{\\beta x}-e^{-\\beta x}}{e^{\\beta x}+e^{-\\beta x}} \\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "la cual graficamos a continuación para diferentes valores de $\\beta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 1.0\n",
    "def g(x):\n",
    "    return np.tanh(beta*x)\n",
    "\n",
    "x = np.linspace(-3,3,1000)\n",
    "for _beta in [0.1,1.0,10.0,100.0]:\n",
    "    beta = _beta\n",
    "    plt.plot(x,np.vectorize(g)(x),label='beta='+str(beta))\n",
    "    \n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"$g_{\\\\beta}$\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta $g$ es diferenciable en $\\mathbb{R}$ y satisface\n",
    "\n",
    "$$ g' = \\beta(1-g^2) $$\n",
    "\n",
    "lo cual, convenientemente, puede ser usado para ahorrar recursos de cómputo.\n",
    "A continuación graficamos ésta derivada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dg(x):\n",
    "    return beta*(1.0-g(x)**2)\n",
    "\n",
    "x = np.linspace(-3,3,1000)\n",
    "for _beta in [2**i for i in range(-2,3)]:\n",
    "    beta = _beta\n",
    "    plt.plot(x,np.vectorize(lambda x:dg(x)/beta)(x),label='beta='+str(beta))\n",
    "    \n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"$g^{'}/\\\\beta$\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "en donde vemos que se transforma en un pico cuyo máximo crece como $\\sim \\beta$ en torno a $x=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un **truco** útil consiste en notar que \n",
    "\n",
    "$$ b + wx = b + \\sum_{i=0}^n w_ix_i $$ \n",
    "\n",
    "puede remplazarse por \n",
    "\n",
    "$$ wx = w_0x_0 + \\sum_{i=1}^n w_ix_i = \\sum_{i=0}^n w_ix_i $$ \n",
    "\n",
    "si introducimos el peso sináptico $w_0=b$ y una $0$-ésima neurona artificial de estado permanente $x_0=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento\n",
    "\n",
    "Consideremos una serie de datos de entrada $x_1,...,x_q$ donde $x_k\\in \\mathbb{R}^{1+n}$ para todo $k=1,...,q$, y una serie de datos de salida $y_1,...,y_q$ donde $y_k\\in \\mathbb{R}$ para todo $k=1,...,q$.\n",
    "Estos datos podrían ser generados por experimentos o sintéticamente, recordando que hay que fijar $x_{k0}=1$ para todo $k$ para poder aprovechar el **truquito**.\n",
    "\n",
    "El objetivo es entrenar los pesos sinápticos de la red hasta que ésta logre aproximar, de la mejor manera posible, la relación entre los datos de entrada y los de salida.\n",
    "Formalmente, buscamos minimizar la suma de errores cuadráticos\n",
    "\n",
    "\\begin{eqnarray}\n",
    "e&:=&\\sum_{k=1}^q (y_k-g(wx_k))^2 \\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "con respecto a $w$.\n",
    "Si pensamos a $e$ como una función $e\\in (\\mathbb{R}^{(1+n)}\\ni w\\to \\mathbb{R})$, podemos intentar minimizarla descendiendo por el gradiente de componentes\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial e}{\\partial w_i}\n",
    "&=& 2\\sum_{k=1}^q (y_k-g(wx_k))\\frac{\\partial}{\\partial w_i}(y_k-g(wx_k)) \\\\\n",
    "&=& -2\\sum_{k=1}^q (y_k-g(wx_k))g'(wx_k)\\frac{\\partial}{\\partial w_i}wx_k \\\\\n",
    "&=& 2\\sum_{k=1}^q (g(wx_k)-y_k)g'(wx_k) x_{ki} \\\\\n",
    "&=& 2\\sum_{k=1}^q (v_k-y_k)u_k x_{ki} \\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "donde $x_{ki}$ es la $i$-ésima componente de la $k$-ésima muestra de entrada $x_k$.\n",
    "Además, por cuestiones prácticas, en la última línea hemos introducido los vectores\n",
    "$v_k := g(wx_k)$\n",
    "y\n",
    "$u_k := g'(wx_k)$.\n",
    "\n",
    "\n",
    "Un sencillo algoritmo de minimización por el gradiente consiste en inicializar $w$ con valores elegidos al azar de alguna distribución centrada en 0, para luego ir actualizandolos iterativamente según las regla\n",
    "\n",
    "$$ w_i \\leftarrow w_i - r \\frac{\\partial e}{\\partial w_i}$$\n",
    "\n",
    "donde $r>0$ es algún valor pequeño que determina la tasa de convergencia.\n",
    "La regla debe aplicarse para todo $i$ en cada iteración, hasta que veamos que el error $e$ deja de decrecer, estacionandose en algún valor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minipráctico\n",
    "\n",
    "**a)** Usar `scikit-learn.datasets.make_classification` para crear un dataset para clasificación con:\n",
    "\n",
    "- 2 características (features)\n",
    "- 2 clases\n",
    "- 200 muestras\n",
    "- sin redundancia\n",
    "- 1 grupo (cluster) por clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "muestras = 200\n",
    "\n",
    "x_ki,y_k = make_classification(\n",
    "    n_features=n,\n",
    "    n_classes=2,\n",
    "    n_samples=muestras,\n",
    "    n_redundant=0,\n",
    "    n_clusters_per_class=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ki.shape,y_k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** Grafique el dataset generado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = {0:'red',1:'blue'}\n",
    "for k in range(muestras):\n",
    "    plt.scatter([x_ki[k,0]],[x_ki[k,1]],c=color[y_k[k]])\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** Implemente un perceptron simple de $2$ neuronas de entrada y una de salida, utilizando\n",
    "\n",
    "$$ g(x) = \\tanh(\\beta x) $$ \n",
    "\n",
    "como función activación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregamos la columna truquito\n",
    "tmp = np.ones((muestras,n+1))\n",
    "tmp[:,1:n+1] = x_ki\n",
    "x_ki = tmp\n",
    "#x_ki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron(w_i,x_i):\n",
    "    return g(np.dot(w_i,x_i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d)** Implemente una funcion que entrene el perceptron. Utilice\n",
    "\n",
    "- $\\beta=1$\n",
    "- $r=0.01$\n",
    "- y $t_{max}=100$ iteraciones\n",
    "\n",
    "El entrenador debe tomar como entrada, entre otras cosas,\n",
    "\n",
    "- el dataset de entrenamiento, i.e. los vectores $x$,$y$, de componentes $x_{ki}$ e $y_k$ correspondiendo a la muestra $k$-ésima y la neurona de entrada $i$-ésima.\n",
    "- el vector de pesos sinápticos $w$\n",
    "\n",
    "y debe retornar un vector de componentes $e_t =$ que indiquen el error cuadrático tras $t$ iteraciones.\n",
    "Durante el proceso de entrenamiento, el entrenador debe modificar el vector $w$ que es pasado como argumento.\n",
    "\n",
    "Grafique $e_t$ vs $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenar(w_i,x_ki,y_k,r,tmax,g):\n",
    "    for t in range(tmax):\n",
    "        v_k = np.vectorize(g)(np.dot(x_ki,w_i))\n",
    "        u_k = beta*(1-v_k**2)\n",
    "        e_k = (v_k-y_k)\n",
    "        e = np.dot(e_k,e_k)\n",
    "        w_i -= 2*r*np.dot(x_ki.T,(v_k-y_k)*u_k) # Notar como usamos producto punto sobre indice k en a_k := (v_k-y_k)*u_k y luego producto escalar entre x_ki y a_k sobre indice k\n",
    "        yield t,e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializamos pesos sinápticos\n",
    "w_i = np.random.normal(size=(n+1))\n",
    "w_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta=1.0\n",
    "r=0.01\n",
    "t_max=100\n",
    "\n",
    "tvec,evec=[],[]\n",
    "for t,e in entrenar(w_i,x_ki,y_k,r,t_max,g):\n",
    "    tvec.append(t)\n",
    "    evec.append(e)\n",
    "plt.plot(tvec,evec)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e)** Grafique el resultado del entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploteamos el resultado del entrenamiento\n",
    "for k in range(muestras):\n",
    "    if perceptron(w_i,x_ki[k,:])>0:\n",
    "        c='blue'\n",
    "    else:\n",
    "        c='red'\n",
    "    plt.scatter([x_ki[k,1]],[x_ki[k,2]],c=c)\n",
    "\n",
    "# Agregamos linea de separacion\n",
    "xmin = np.min(x_ki[:,1])\n",
    "xmax = np.max(x_ki[:,1])\n",
    "xs = np.linspace(xmin,xmax,100)\n",
    "def yy(xx):\n",
    "    w0 = w_i[0]\n",
    "    w1 = w_i[1]\n",
    "    w2 = w_i[2]\n",
    "    return -w0/w2 - w1/w2*xx\n",
    "plt.plot(xs,np.vectorize(yy)(xs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**f)** Repita todo lo anterior probando con otras funciones de activación, valores de $r$ y de $\\beta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
