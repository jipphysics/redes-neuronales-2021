{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refs.\n",
    "\n",
    "1. Introduction to the theory of neuronal computation, Hertz, Krogh, Palmer (1991)\n",
    "\n",
    "2. https://machinelearningmastery.com/implement-perceptron-algorithm-scratch-python/\n",
    "\n",
    "3. https://towardsdatascience.com/perceptron-explanation-implementation-and-a-visual-example-3c8e76b4e2d1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teoría\n",
    "\n",
    "### Múltiples capas y neuronas de salida\n",
    "\n",
    "Consideramos un perceptrón de $m+1$ capas y de $n_l$ neuronas en la $l$-ésima capa, siendo $n_0$ el número de neuronas en la capa de entrada y $n_m$ el número de neuronas en la capa de salida.\n",
    "\n",
    "La componente $V_{i}^{(l)}\\in\\mathbb{R}$ representa el estado de la $i$-ésima neurona en la $l$-ésima capa,\n",
    "la componente $w_{ji}^{(l)}\\in\\mathbb{R}$ el peso sináptico saliendo de la $i$-ésima neurona en la $(l-1)$-ésima capa y entrando a la $j$-ésima neurona de la $l$-ésima capa.\n",
    "De esta manera, el estado $V_{j}^{(l+1)}$ de la $j$-ésima neurona de la $(l+1)$-ésima capa viene determinado por\n",
    "\n",
    "$$ V_{j}^{(l+1)} = g(h_{j}^{l+1}) \\;\\;\\; (1)$$\n",
    "\n",
    "donde\n",
    "\n",
    "$$ h_{j}^{(l+1)} := \\sum_i w_{ji}^{(l+1)}V_{i}^{(l)} \\;\\;\\; (2)$$\n",
    "\n",
    "Asuminos que en cada capa $l<m$ existe una neurona de estado $V_{0}^{(l)}=1$ con el fin de implementar el truquito que nos permite remplazar el umbral en cada neurona por un peso sináptico.\n",
    "\n",
    "Notar que, con excepción de las neuronas de entrada en la capa $l=0$, todas las neuronas usan la misma función activación $g$.\n",
    "Existen muchas funciones de activación, pero nos enfocaremos en usar\n",
    "\n",
    "\\begin{eqnarray}\n",
    "g(x)\n",
    "&:=& \\tanh(\\beta x) \\\\\n",
    "&=& \\frac{e^{\\beta x}-e^{-\\beta x}}{e^{\\beta x}+e^{-\\beta x}} \\;\\;\\; (3) \\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "cuya derivada convenientemente satisface $g' = \\beta(1-g^2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento\n",
    "\n",
    "Consideraremos una serie de datos de $\\mu=1,...,p$ muestras, de entradas $\\xi_{\\mu i}$ y salidas $\\zeta_{\\mu j}$ en $\\mathbb{R}$.\n",
    "Es decir, $x_{\\mu i}$ representa el valor de entrada de la $i$-ésima neurona en la capa $l=0$ y $\\zeta_{\\mu j}$ el valor pretendido de salida en la $j$-ésima neurona de la capa $l=m$.\n",
    "\n",
    "El objetivo es entrenar los pesos sinápticos representados por $w$, de manera que la salida \n",
    "\n",
    "$$ V_{\\mu j}^{(m)} =: O_{\\mu j} \\approx \\zeta_{\\mu j}\\;\\;\\; (4) $$ \n",
    "\n",
    "cuando la entrada es\n",
    "\n",
    "$$ V_{\\mu i}^{(0)} = \\xi_{\\mu i} \\;\\;\\; (5) $$\n",
    "\n",
    "para todo $i=1,...,n_0$, $j=1,...,n_m$ y muestra $\\mu=1,...,p$.\n",
    "Aquí, $V_{\\mu j}^{(m)}$ es el valor que adopta la $j$-ésima neurona en la capa $m$ cuando la red es expuesta a la $\\mu$-ésima entrada.\n",
    "\n",
    "Formalmente, buscaremos minimizar el error cuadrático\n",
    "\n",
    "\\begin{eqnarray}\n",
    "E := \\sum_{\\mu j} \\frac{1}{2}(\\zeta_{\\mu j}-O_{\\mu j})^2 \\;\\;\\; (6)\n",
    "\\end{eqnarray}\n",
    "\n",
    "con respecto a $w$.\n",
    "Aquí, debemos recordar que $V_{\\mu j}^{(m)}$ depende de los pesos sinápticos $w$ y de las entradas $x$ (ver Ecs. (1-5)).\n",
    "\n",
    "Como algoritmo de minimización, utilizaremos una versión del método de descenso por el gradiente llamado **back-propagation**.\n",
    "Combiene simplificar notación y definir el operador derivada parcial $\\partial_x$ por $\\partial_x f := \\frac{\\partial }{\\partial x}f$ para cualquier funcion $f:\\mathbb{R}\\to \\mathbb{R}$.\n",
    "La idea nace de calcular el gradiente de $E$ respecto de $w$, capa por capa.\n",
    "Comenzamos por calcular componentes del gradiente correspondientes a pesos sinápticos de la última capa\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\partial_{w_{rs}^{(m)}}E\n",
    "&=&\\sum_{\\mu j} \\frac{1}{2}\\partial_{w_{rs}^{(m)}}(\\zeta_{\\mu j}-O_{\\mu j})^2\\\\\n",
    "&=&\\sum_{\\mu j} \\frac{1}{2}\\partial_{w_{rs}^{(m)}}(\\zeta_{\\mu j}-V_{\\mu j}^{(m)})^2\\\\\n",
    "&=&\\sum_{\\mu j} (\\zeta_{\\mu j}-V_{\\mu j}^{(m)})\\partial_{w_{rs}^{(m)}}(\\zeta_{\\mu j}-V_{\\mu j}^{(m)}) \\;\\;\\; (7)\\\\\n",
    "&=&-\\sum_{\\mu j}(\\zeta_{\\mu j}-V_{\\mu j}^{(m)})\\partial_{w_{rs}^{(m)}}V_{\\mu j}^{(m)}\\\\\n",
    "&=&\\sum_{\\mu j}(V_{\\mu j}^{(m)}-\\zeta_{\\mu j})\\partial_{w_{rs}^{(m)}}g(h_{\\mu j}^{(m)})\\\\\n",
    "&=&\\sum_{\\mu j}(V_{\\mu j}^{(m)}-\\zeta_{\\mu j})g'(h_{\\mu j}^{(m)})\\partial_{w_{rs}^{(m)}}h_{\\mu j}^{(m)}\\\\\n",
    "&=&\\sum_{\\mu j}(V_{\\mu j}^{(m)}-\\zeta_{\\mu j})g'(h_{\\mu j}^{(m)})\\partial_{w_{rs}^{(m)}}\\sum_i w_{ji}^{(m)}V_{\\mu i}^{(m-1)}\\\\\n",
    "&=&\\sum_{\\mu ji}(V_{\\mu j}^{(m)}-\\zeta_{\\mu j})g'(h_{\\mu j}^{(m)})\\big(\\delta_{mm}\\delta_{rj}\\delta_{si}V_{\\mu i}^{(m-1)}+w_{ji}^{(m)}\\partial_{w_{rh}^{(m)}}V_{\\mu i}^{(m-1)}\\big)\\\\\n",
    "&=&\\sum_{\\mu ji}(V_{\\mu j}^{(m)}-\\zeta_{\\mu j})g'(h_{\\mu j}^{(m)})\\big(\\delta_{rj}\\delta_{si}V_{\\mu i}^{(m-1)}+w_{ji}^{(m)}0\\big)\\\\\n",
    "&=&\\sum_{\\mu }(V_{\\mu r}^{(m)}-\\zeta_{\\mu r})g'(h_{\\mu r}^{(m)})V_{\\mu s}^{(m-1)}\\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "donde usamos que $\\partial_{w_{rs}^{(m)}}V_{i}^{(m-1)}=0$ porque $V_{i}^{(l)}$ no depende de $w_{rs}^{(f)}$ para todo $f>l$ como puede deducirse de mirar la figura\n",
    "\n",
    "<img src=\"fig1.png\" width=\"400\">\n",
    "\n",
    "Luego, introduciendo\n",
    "\n",
    "$$ d^{(m)}_{\\mu r} := (O_{\\mu r}-\\zeta_{\\mu r})g'(h_{\\mu r}^{(m)}) \\;\\;\\; (8)$$\n",
    "\n",
    "reescribimos el anterior resultado como\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\partial_{w_{rs}^{(m)}}E\n",
    "&=&\\sum_{\\mu }d^{(m)}_{\\mu r}V_{\\mu s}^{(m-1)} \\;\\;\\; (9)\\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "la cual convenientemente constituye una expresión en donde no aparecen derivadas explícitamente.\n",
    "\n",
    "Luego, nos interesa calcular las componentes del gradiente para pesos sinápticos de alguna capa oculta arbitrara\n",
    "\\begin{eqnarray}\n",
    "\\partial_{w_{rs}^{(m-p)}}E\n",
    "&=&\\sum_{\\mu j}(V_{\\mu j}^{(m)}-\\zeta_{\\mu j})g'(h_{\\mu j}^{(m)})\\partial_{w_{rs}^{(m-p)}}h_{\\mu j}^{(m)} \\;\\;\\; (10) \\\\\n",
    "&=&\\sum_{\\mu j}d^{(m)}_{\\mu r}\\partial_{w_{rs}^{(m-p)}}\\sum_i w_{ji}^{(m)}V_{\\mu i}^{(m-1)}\\\\\n",
    "&=&\\sum_{\\mu ji}d^{(m)}_{\\mu r}w_{ji}^{(m)}\\partial_{w_{rs}^{(m-p)}}V_{\\mu i}^{(m-1)}\\\\\n",
    "&=&\\sum_{\\mu ji}d^{(m)}_{\\mu r}w_{ji}^{(m)}\\partial_{w_{rs}^{(m-p)}}g(h_{\\mu i}^{(m-1)})\\\\\n",
    "&=&\\sum_{\\mu ji}d^{(m)}_{\\mu r}w_{ji}^{(m)}g'(h_{\\mu i}^{(m-1)})\\partial_{w_{rs}^{(m-p)}}h_{\\mu i}^{(m-1)}\\\\\n",
    "&=&\\sum_{\\mu i}g'(h_{\\mu i}^{(m-1)})\\big(\\sum_j d^{(m)}_{\\mu r}w_{ji}^{(m)}\\big)\\partial_{w_{rs}^{(m-p)}}h_{\\mu i}^{(m-1)}\\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "Aquí, conviene introducir una definición análoga a la de los $d^{(m)}_{\\mu i}$, pero para la capa $(m-1)$-ésima\n",
    "\n",
    "$$ d^{(m-1)}_{\\mu i} := g'(h_{\\mu i}^{(m-1)})\\sum_j d^{(m)}_{\\mu r}w_{ji}^{(m)} \\;\\;\\; (11) $$\n",
    "\n",
    "así podemos reescribir la expresión de la última línea en la Ec. (10) como\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\partial_{w_{rs}^{(m-p)}}E\n",
    "&=&\\sum_{\\mu i}g'(h_{\\mu i}^{(m-1)})\\big(\\sum_j d^{(m)}_{\\mu r}w_{ji}^{(m)}\\big)\\partial_{w_{rs}^{(m-p)}}h_{\\mu i}^{(m-1)} \\;\\;\\; (12)\\\\\n",
    "&=&\\sum_{\\mu i}d^{(m-1)}_{\\mu i}\\partial_{w_{rs}^{(m-p)}}h_{\\mu i}^{(m-1)}\\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "Si bien en esta expresión aún aparecen derivadas explícitas, procederemos por inducción para eliminarlas.\n",
    "Para ello, definimos\n",
    "\n",
    "$$ d^{(m-f)}_{\\mu i} := g'(h_{\\mu i}^{(m-f)})\\sum_j d^{(m-f+1)}_{\\mu r}w_{ji}^{(m-f+1)} \\;\\;\\; (13) $$\n",
    "\n",
    "y asumimos la hipótesis inductiva\n",
    "\n",
    "$$ \\partial_{w_{rs}^{(m-p)}}E = \\sum_{\\mu i}d^{(m-f)}_{\\mu i}\\partial_{w_{rs}^{(m-p)}}h_{ki}^{(m-f)} \\;\\;\\; (14) $$\n",
    "\n",
    "para cualquier $f$ tal que $0<f<p$.\n",
    "En particular, notar que la hipótesis inductiva vale para $f=1$, tal como se deduce en la Ec. (12).\n",
    "\n",
    "Luego, si la hipótesis inductiva vale para $f$, entonces el siguiente cálculo\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\partial_{w_{rs}^{(m-p)}}E\n",
    "&=&\\sum_{\\mu i}d^{(m-f)}_{\\mu i}\\partial_{w_{rs}^{(m-p)}}h_{\\mu i}^{(m-f)} \\;\\;\\; (15)\\\\\n",
    "&=&\\sum_{\\mu i}d^{(m-f)}_{\\mu i}\\partial_{w_{rs}^{(m-p)}}\\sum_j w_{ij}^{(m-f)}V_{\\mu j}^{(m-f-1)}\\\\\n",
    "&=&\\sum_{\\mu ij}d^{(m-f)}_{\\mu i}w_{ij}^{(m-f)}\\partial_{w_{rs}^{(m-p)}}V_{\\mu j}^{(m-f-1)}\\\\\n",
    "&=&\\sum_{\\mu ij}d^{(m-f)}_{\\mu i}w_{ij}^{(m-f)}\\partial_{w_{rs}^{(m-p)}}g(h_{\\mu j}^{(m-f-1)})\\\\\n",
    "&=&\\sum_{\\mu ij}d^{(m-f)}_{\\mu i}w_{ij}^{(m-f)}g'(h_{\\mu j}^{(m-f-1)})\\partial_{w_{rs}^{(m-p)}}h_{\\mu j}^{(m-f-1)}\\\\\n",
    "&=&\\sum_{\\mu j}g'(h_{\\mu j}^{(m-f-1)})\\big(\\sum_i d^{(m-f)}_{\\mu i}w_{ij}^{(m-f)}\\big)\\partial_{w_{rs}^{(m-p)}}h_{\\mu j}^{(m-f-1)}\\\\\n",
    "&=&\\sum_{\\mu j}d^{(m-f-1)}_{\\mu j}\\partial_{w_{rs}^{(m-p)}}h_{\\mu j}^{(m-f-1)}\\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "muestra que la hipótesis inductiva vale para $f+1$.\n",
    "\n",
    "La hipótesis inductiva puede aplicarse recursivamente hasta eventualmente llegar al caso $f=p$ en donde deja de valer, pero en cambio se obtiene\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\partial_{w_{rs}^{(m-p)}}E\n",
    "&=&\\sum_{\\mu i}d^{(m-p)}_{\\mu i}\\partial_{w_{rs}^{(m-p)}}\\sum_j w_{ij}^{(m-p)}V_{\\mu j}^{(m-q-1)} \\;\\;\\; (16)\\\\\n",
    "&=&\\sum_{\\mu ij}d^{(m-p)}_{\\mu i}\\delta_{ri}\\delta_{sj}V_{\\mu j}^{(m-p-1)}\\\\\n",
    "&=&\\sum_{\\mu }d^{(m-p)}_{\\mu r}V_{\\mu s}^{(m-p-1)}\\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "la cual es una expresión en donde no aparece explícitamente derivada alguna, y es el resultado al cuál queríamos llegar.\n",
    "\n",
    "En otras palabras, vemos que puede obtenerse una expresión en donde no aparecen derivadas explícitas para cada componente $\\partial_{w_{rs}^{(l)}}E$ del gradiente.\n",
    "Por otro lado, es importante resaltar que estas expresiones dependen de los $d^{(l)}_{\\mu r}$, los cuales están recursivamente determinados por la Ec. (13) en donde tampoco aparecen derivadas explíticas, salvo las de la función activación $g$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algoritmo\n",
    "\n",
    "El principal objetivo es minimizar el error $E$ usando el algoritmo de descenso por el gradiente, iterando\n",
    "\n",
    "$$ w^{(l)}_{ji}(t+1) = w^{(l)}_{ji}(t) - \\eta \\partial_{w^{(l)}_{ji}}E(t) \\;\\;\\; (17)$$\n",
    "\n",
    "sobre $t$ para todo $l$ y $ji$.\n",
    "En esta notebook, iniciaremos la iteración desde una condición inicial aleatoria $w^{(l)}_{ji}(t=0) \\sim \\mathcal{N}$ donde $\\mathcal{N}$ representa una distribución normal de media 0 y varianza 1.\n",
    "\n",
    "Para calcular los $\\partial_{w^{(l)}_{ji}}e$ necesitamos calcular los $V^{(l)}_{\\mu i}$ y $d^{(l)}_{\\mu i}$.\n",
    "Ambos requieren del cálculo de los $h^{(l)}_{\\mu i}$.\n",
    "Observando la Ec. (13), nos damos cuenta que es necesario calcular primero los $d^{(m)}_{\\mu i}$, luego los $d^{(m-1)}_{\\mu i}$ y así hasta poder calcular los $d^{(1)}_{\\mu i}$.\n",
    "\n",
    "Luego de inicializar\n",
    "\n",
    "$$ V^{(0)}_{\\mu i} = \\xi_{\\mu i} $$\n",
    "\n",
    "el cálculo se puede realizar en dos fases. \n",
    "Primero la fase **forward**, en donde calculamos los $V^{(l)}_{\\mu i}$ y $h^{(l)}_{\\mu i}$ capa por capa en orde creciente en $l$.\n",
    "Más precisamente\n",
    "\n",
    "   \n",
    "1. Iterando sobre $l=1,2,...,m$, calculamos para cada $l$:\n",
    "\n",
    "    i. $h^{(l)}_{\\mu i} = \\sum_j w^{(l)}_{ij} V^{(l-1)}_{\\mu j}$\n",
    "    \n",
    "    ii. $V^{(l)}_{\\mu i} = g(h^{(l)}_{\\mu i})$\n",
    "\n",
    "Luego la fase **backwards**, en donde calculamos los $d^{(l)}_{ki}$, $\\partial_{w^{(l)}_{ji}}E$ y actualizamos los $w^{(l)}_{ji}$ en orden decreciente en $l$.\n",
    "Más precisamente\n",
    "\n",
    "1. Calculamos\n",
    "\n",
    "    i. $ d_{\\mu i}^{(m)} = g'(h^{(m)}_{\\mu i})(V_{\\mu i}^{(m)}-\\zeta_{\\mu i}) $\n",
    "\n",
    "    y actualizamos\n",
    "\n",
    "    ii. $ w^{(m)}_{ji} -= \\eta \\sum_{\\mu} d^{(m)}_{\\mu j} V^{(m-1)}_{\\mu i} $\n",
    "\n",
    "2. Luego, iterando sobre $l=m-1,m-2,...,1$, calculamos para cada $l$:\n",
    "\n",
    "    i. $ d_{\\mu i}^{(l)} = g'(h^{(l)}_{\\mu i}) \\sum_j d^{(l+1)}_{\\mu j} w^{(l+1)}_{ji} $\n",
    "    \n",
    "    y actualizamos\n",
    "    \n",
    "    ii. $ w^{(l)}_{ji} -= \\sum_{\\mu} d^{(l)}_{\\mu j} V^{(l-1)}_{\\mu i} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minipráctico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)** Implemente un perceptron de $1+m=3$ capas, una de entrada de $n_0=2$ neuronas, una oculta de $n_1=2$ neuronas y una de salida de $n_2=1$ neurona, utilizando $g(x) = \\tanh(\\beta x)$ como función activación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta=1.0\n",
    "def g(x):\n",
    "    return np.tanh(beta*x)\n",
    "def dg(x):\n",
    "    return beta*(1.0-g(x)**2)\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self,n,x,z,epocas=100,eta=0.01,g=g,dg=dg):\n",
    "        \"\"\"\n",
    "        n: N^(1+m) lista de tamaños de capas (sin incluir neuronas truquito)\n",
    "        x: R^(q,n[0])\n",
    "        z: R^(q,n[m])\n",
    "        epocas: N\n",
    "        beta: R\n",
    "        eta: R\n",
    "        \"\"\"\n",
    "        m=len(n)-1 # n[0],n[1],...,n[m]\n",
    "        p,_=x.shape\n",
    "        self.n=n\n",
    "        self.m=m        \n",
    "        self.beta=beta\n",
    "        V = [None]*(1+m)\n",
    "        h = [None]*(1+m)\n",
    "        d = [None]*(1+m)\n",
    "        w = [None]*(1+m)\n",
    "        # Inicializamos pesos. Las primeras capas tienen en cuenta neuronas truquito. La ultima no.\n",
    "        for l in range(1,m):\n",
    "            w[l]=np.random.normal(size=(n[l]+1,n[l-1]+1))\n",
    "        w[m]=np.random.normal(size=(n[m],n[m-1]+1))\n",
    "        # Inicializamos activaciones, preactivaciones y diferencias\n",
    "        V[0] = np.ones((p,n[0]+1)) # Inicializamos y agregamos truquito a V^0_ki\n",
    "        V[0][:,1:] = x[:,:]\n",
    "        # Entrenamos\n",
    "        self.list_E=[]\n",
    "        for t in range(epocas):\n",
    "            # Forward\n",
    "            for l in range(1,m+1):\n",
    "                h[l] = np.tensordot(V[l-1],w[l],axes=([1],[1]))\n",
    "                V[l] = np.vectorize(g)(h[l])\n",
    "            # Calculamos error cuadratico y reportamos\n",
    "            err = V[m]-z.reshape(V[m].shape)\n",
    "            self.list_E.append(np.dot(err.flatten(),err.flatten()))\n",
    "            # Backward\n",
    "            d[m] = np.vectorize(dg)(h[m])*err\n",
    "            w[m] -= eta*np.tensordot(d[m],V[m-1],axes=([0],[0]))\n",
    "            for l in range(m-1,0,-1):\n",
    "                d[l] = np.vectorize(dg)(h[l])*np.tensordot(d[l+1],w[l+1],axes=([1],[0]))\n",
    "                w[l] -= eta*np.tensordot(d[l],V[l-1],axes=([0],[0]))\n",
    "        # grabamos los pesos como miembro de la clase\n",
    "        self.w=w\n",
    "    def __call__(self,x):\n",
    "        n=self.n\n",
    "        m=self.m\n",
    "        w=self.w\n",
    "        V = [None]*(1+m)\n",
    "        h = [None]*(1+m)\n",
    "        V[0] = np.ones(n[0]+1)\n",
    "        V[0][1:] = x\n",
    "        for l in range(1,m+1):\n",
    "            h[l] = np.dot(w[l],V[l-1])\n",
    "            V[l] = np.vectorize(g)(h[l])\n",
    "        return V[m]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** Usar `scikit-learn.datasets.make_classification` para crear un dataset para clasificación con:\n",
    "\n",
    "- 2 características (features)\n",
    "- 2 clases\n",
    "- 100 muestras\n",
    "- sin redundancia\n",
    "- 1 grupo (cluster) por clase\n",
    "\n",
    "Grafique el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "n0 = 2  # n[0]\n",
    "p = 100 # mu=0,1,...,p-1 donde p = número de muestras.\n",
    "n_classes = 2 \n",
    "x,y = make_classification(\n",
    "    n_features=n0,\n",
    "    n_classes=n_classes,\n",
    "    n_samples=p,\n",
    "    n_redundant=0,\n",
    "    n_clusters_per_class=1\n",
    ")\n",
    "\n",
    "print(\"x.shape=\",x.shape,\"y.shape=\",y.shape,sep=\"\")\n",
    "\n",
    "color = {0:'red',1:'blue',2:'green',3:'cyan'}\n",
    "for k in range(x.shape[0]):\n",
    "    plt.scatter([x[k,0]],[x[k,1]],c=color[y[k]])\n",
    "plt.title(\"datos\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** Entrene el perceptron en el dataset generado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Perceptron([2,2,1],x,y)\n",
    "plt.plot(range(len(p.list_E)),p.list_E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d)** Grafique el resultado del entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploteamos el resultado del entrenamiento\n",
    "def heaviside(x):\n",
    "    if x>0.5:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "num_error = 0\n",
    "for k in range(x.shape[0]):\n",
    "    y_pred=p(x[k,:])[0]\n",
    "    y_pred_bin = heaviside(y_pred)\n",
    "    if y[k]!=y_pred_bin:\n",
    "        num_error += 1\n",
    "    #print(\"k=\",k,\" y=\",y[k],\" y_pred_bin=\",y_pred_bin,\" y_pred=\",y_pred,sep=\"\")\n",
    "    c_pred = color[y_pred_bin]\n",
    "    c_true = color[y[k]]\n",
    "    plt.scatter([x[k,0]],[x[k,1]],color=c_pred,marker='.',edgecolors=c_true,linewidth=1,s=250)\n",
    "print(\"num_error=\",num_error,sep=\"\")\n",
    "    \n",
    "xmin = np.min(x[:,0])\n",
    "xmax = np.max(x[:,0])\n",
    "xs = np.linspace(xmin,xmax,100)\n",
    "ymax = np.max(x[:,1])\n",
    "ymin = np.min(x[:,1])\n",
    "plt.title(\"prediccion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e)** El problema de la compuerta **XOR**.\n",
    "\n",
    "La compuerta XOR viene dada por\n",
    "\n",
    "    0 0 -> 0\n",
    "    0 1 -> 1\n",
    "    1 0 -> 1    \n",
    "    1 1 -> 0\n",
    "    \n",
    "Muestre que un perceptrón simple no puede aprender la compuerta XOR, mientras que un perceptrón con sólo una capa oculta de dos neuronas (i.e. una capa de entrada de dos neuronas, una oculta de dos neuronas y una de salida de 1 neurona) si puede."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos los datos\n",
    "x = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y = np.array([0,1,1,0])\n",
    "for k in range(x.shape[0]):\n",
    "    plt.scatter([x[k,0]],[x[k,1]],c=color[y[k]])\n",
    "plt.title(\"datos\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamos el perceptron simple\n",
    "p_simple = Perceptron([2,1],x,y,epocas=4000)\n",
    "plt.plot(range(len(p_simple.list_E)),p_simple.list_E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(x.shape[0]):\n",
    "    y_pred=p_simple(x[k,:])[0]\n",
    "    y_pred_bin = heaviside(y_pred)\n",
    "    print(\"x=\",x[k,:],\" y=\",y[k],\" y_pred_bin=\",y_pred_bin,\" y_pred=\",y_pred,sep=\"\")\n",
    "    c_pred = color[y_pred_bin]\n",
    "    c_true = color[y[k]]\n",
    "    plt.scatter([x[k,0]],[x[k,1]],color=c_pred,marker='.',edgecolors=c_true,linewidth=1,s=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamos el perceptron con una capa oculta.\n",
    "p = Perceptron([2,2,1],x,y,epocas=4000)\n",
    "plt.plot(range(len(p.list_E)),p.list_E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(x.shape[0]):\n",
    "    y_pred=p(x[k,:])[0]\n",
    "    y_pred_bin = heaviside(y_pred)\n",
    "    print(\"x=\",x[k,:],\" y=\",y[k],\" y_pred_bin=\",y_pred_bin,\" y_pred=\",y_pred,sep=\"\")\n",
    "    c_pred = color[y_pred_bin]\n",
    "    c_true = color[y[k]]\n",
    "    plt.scatter([x[k,0]],[x[k,1]],color=c_pred,marker='.',edgecolors=c_true,linewidth=1,s=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
